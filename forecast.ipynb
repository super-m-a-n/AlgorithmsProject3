{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forecast.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TA-THfawj5lv"
      },
      "outputs": [],
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run .py files"
      ],
      "metadata": {
        "id": "131n3-GpJ6oF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"/content/forecast.py\" -d data/nasdaq2007_17.csv -n 3"
      ],
      "metadata": {
        "id": "cg0WhWCb1CVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## forecast_utils.py"
      ],
      "metadata": {
        "id": "-5MQxw2oJib1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" file : forecast_utils.py \"\"\"\n",
        "\"\"\" helper functions for forecast.py main\"\"\"\n",
        "\n",
        "\"\"\" general imports \"\"\"\n",
        "import sys\n",
        "import string\n",
        "\n",
        "\"\"\" imports for plots, dataframes, numpy \"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\"\"\" keras neural network imports \"\"\"\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\"\"\" sklearn imports \"\"\"\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def config_model(input_size):\n",
        "    \"\"\" configure and return RNN-LSTM model to be trained \"\"\"\n",
        "    model = Sequential()\n",
        "    # Adding the first input LSTM layer\n",
        "    model.add(LSTM(units = 64, return_sequences = True, input_shape = (input_size, 1)))\n",
        "    # Dropout layer to avoid overfitting\n",
        "    model.add(Dropout(0.3))\n",
        "    # Adding a second LSTM layer\n",
        "    model.add(LSTM(units = 64))\n",
        "    # Dropout layer to avoid overfitting\n",
        "    model.add(Dropout(0.3))\n",
        "    # Adding the output layer\n",
        "    model.add(Dense(units = 1))\n",
        "    # Compiling the RNN by determinng optimizer and loss function\n",
        "    model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\n",
        "    return model\n",
        "\n",
        "\n",
        "def plot_loss(history):\n",
        "    \"\"\" plot the train loss vs test loss learning curve for given training history \"\"\"\n",
        "    plt.figure(figsize=(16, 6), dpi=120)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model train vs test loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_series(time_points, original_series, predicted_series, window, series_name):\n",
        "    \"\"\" plot the original time series vs predicted time series \"\"\"\n",
        "    plt.figure(figsize=(16, 6), dpi=120)\n",
        "    plt.plot(time_points, original_series, color = 'red', label = 'Original Time Series Values')\n",
        "    plt.plot(time_points, predicted_series, color = 'blue', label = 'Predicted Time Series Values')\n",
        "    plt.xticks(np.arange(time_points[0], time_points[-1], window))\n",
        "    plt.title(\"Time Series Prediction (name of series --> %s)\" % (series_name))\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Time Series Value')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def create_dataset(input_set, num_of_series, num_of_steps, w):\n",
        "    \"\"\" creates the x_set, y_set (x_train / y_train or x_test / y_test ) out of the input_set\n",
        "        input_set : 2D array -> columns are different time series, rows are time series values across time\n",
        "        num_of_series : number of series in input_set\n",
        "        num_of_steps  : number of steps = size of x_set and y_set\n",
        "        w : window of sampling \"\"\"\n",
        "\n",
        "    # x_set will contain windows of w consecutive values for all time series of input_set\n",
        "    x_set = []\n",
        "    # y_set will contain the next value of corresponding time series for each of the windows of x_set\n",
        "    y_set = []\n",
        "\n",
        "    for series_index in range(num_of_series):\n",
        "        for i in range(w, num_of_steps + w):\n",
        "            # append next window of w consecutive values of time series in x_set\n",
        "            x_set.append(input_set[i-w:i, series_index])\n",
        "            # append next value of time series outside current window in y_set\n",
        "            y_set.append(input_set[i, series_index])\n",
        "\n",
        "    # convert to np arrays\n",
        "    x_set, y_set = np.array(x_set), np.array(y_set)\n",
        "    # reshape x_set to a tensor of size (#time_series * num_of_steps, window, 1)\n",
        "    x_set = np.reshape(x_set, (x_set.shape[0], x_set.shape[1], 1))\n",
        "\n",
        "    return x_set, y_set\n",
        "\n",
        "\n",
        "def execute(series_values, series_names, w, load_trained_model, series_to_use):\n",
        "    \"\"\" function where main work gets done\n",
        "        series_values : 2D array -> columns are different time series, rows are time series values across time\n",
        "        series_names  : 1D array -> rows have the names of the time series\n",
        "        w : window of sampling\n",
        "        load_trained_model : if True, a pre trained model will be loaded, otherwise a new model will be trained\n",
        "        series_to_use   : if not None, training/loading is done per given series, otherwise per entire set of series \"\"\"\n",
        "\n",
        "    # train test split 80-20 rule (the first 80% of time series values will be used for the training, rest 20% for testing)\n",
        "    train_set_size = math.floor((series_values.shape[0])*0.8)\n",
        "\n",
        "    # save train set after split\n",
        "    train_set = series_values[:train_set_size]\n",
        "\n",
        "    # save test set after split\n",
        "    test_set = series_values[train_set_size:]\n",
        "\n",
        "    # scale train set manually using min max scaling\n",
        "    scaled_train_set = (train_set - train_set.min()) / (train_set.max() - train_set.min())\n",
        "\n",
        "    num_of_series = series_values.shape[1]\n",
        "\n",
        "    # get x_train, y_train\n",
        "    x_train, y_train = create_dataset(scaled_train_set, num_of_series, train_set.shape[0] - w, w)\n",
        "\n",
        "    if load_trained_model is False:\n",
        "        # configure new RNN-LSTM model\n",
        "        model = config_model(x_train.shape[1])\n",
        "    else:\n",
        "        if series_to_use is not None:\n",
        "            # load pre trained RNN-LSTM model for series\n",
        "            model = load_model('/content/models/forecast/model_' + str(series_to_use) + '.h5')\n",
        "        else:\n",
        "            # load pre trained RNN-LSTM model for entire set of series\n",
        "            model = load_model('/content/models/forecast/big_model.h5')\n",
        "\n",
        "    # getting all the values of all selected time series starting from the last window before the test set\n",
        "    inputs = series_values[series_values.shape[0] - test_set.shape[0] - w:]\n",
        "    # normalize with minmax normalization based on train set\n",
        "    inputs = (inputs - train_set.min()) / (train_set.max() - train_set.min())\n",
        "\n",
        "    # get x_test, y_test\n",
        "    x_test, y_test = create_dataset(inputs, num_of_series, test_set.shape[0], w)\n",
        "\n",
        "    # if load_trained_model option was not given, train new model\n",
        "    if load_trained_model is False:\n",
        "        # train model by fitting it to the training set\n",
        "        history = model.fit(x_train, y_train, epochs = 60, batch_size = 256, validation_data=(x_test, y_test), verbose=1)\n",
        "        # plot the train loss vs test loss learning curve for given training history\n",
        "        plot_loss(history)\n",
        "\n",
        "        if series_to_use is not None:\n",
        "            # save trained RNN-LSTM model of series, for future use\n",
        "            model.save('/content/models/forecast/model_' + str(series_to_use) + '.h5')\n",
        "        else:\n",
        "            # save trained RNN-LSTM model of entire set of series, for future use\n",
        "            model.save('/content/models/forecast/big_model.h5')\n",
        "\n",
        "    # time axis\n",
        "    time_points = [index + 1 for index in range(train_set.shape[0], series_values.shape[0])]\n",
        "\n",
        "    # for each of the selected time series make the original vs predicted series plot\n",
        "    for series_index in range(num_of_series):\n",
        "        # get name for current time series\n",
        "        series_name = series_names[series_index, 0]\n",
        "\n",
        "        # get original series (test set) for current time series\n",
        "        original_series = test_set[:, series_index:series_index+1]\n",
        "\n",
        "        # get x_test for current time series\n",
        "        x_test_series = x_test[test_set.shape[0] * series_index : test_set.shape[0] * (series_index+1), :, :]\n",
        "\n",
        "        # get predictions of future time series values (time series values on test set) (for current time series)\n",
        "        predicted_series = model.predict(x_test_series)\n",
        "\n",
        "        # get unscaled predictions of time series values (manual inverse transform)\n",
        "        predicted_series = predicted_series * (train_set.max() - train_set.min()) + train_set.min()\n",
        "\n",
        "        # plot the original time series vs predicted time series\n",
        "        plot_series(time_points, original_series, predicted_series, w, series_name)\n"
      ],
      "metadata": {
        "id": "fMD8-44iJTN1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## forecast.py"
      ],
      "metadata": {
        "id": "Cvx9D8k4JpoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" file : forecast.py \"\"\"\n",
        "\n",
        "\"\"\" general imports \"\"\"\n",
        "#import argparse     # for command line argument parsing\n",
        "#import sys\n",
        "#import string\n",
        "\n",
        "\"\"\" imports for dataframes, numpy \"\"\"\n",
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "#import math\n",
        "\n",
        "\"\"\" import functions from forecast_utils.py \"\"\"\n",
        "#from forecast_utils import execute\n",
        "\n",
        "# main functionality\n",
        "\n",
        "# parse command line input\n",
        "#parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('-d', help=\"Specify input dataset file path\", type=str, required=True)\n",
        "#parser.add_argument('-n', help=\"Specify index of time series to forecast\", type=int, required=True)\n",
        "#parser.add_argument('-load', help=\"Specify use of pre-trained models\", dest='load', action='store_true', required=False)\n",
        "#parser.add_argument('-train', help=\"Specify training of models\", dest='load', action='store_false', required=False)\n",
        "#parser.set_defaults(load=True)\n",
        "#args = parser.parse_args()\n",
        "\n",
        "#if args.n < 1:\n",
        "#    sys.exit(\"Invalid time series index. Index should be positive integer\\n\")\n",
        "\n",
        "# save arguments\n",
        "dataset_path = 'data/nasdaq2007_17.csv'\n",
        "num_of_series = 5\n",
        "\n",
        "# load pre trained model or train new model\n",
        "load_trained_model = True\n",
        "\n",
        "# Load and peek the input dataset csv file\n",
        "dataset = pd.read_csv(dataset_path, sep='\\t', lineterminator='\\n', header=None)\n",
        "\n",
        "# keep names of time series in seperate dataframe and convert it to an array\n",
        "series_names = pd.DataFrame(dataset, columns=[dataset.columns[0]]).values\n",
        "\n",
        "# specify columns to keep (all but the first one)\n",
        "cols = [False]\n",
        "rest_cols= [True for x in range(dataset.shape[1]-1)]\n",
        "cols.extend(rest_cols)\n",
        "\n",
        "# set window\n",
        "w = 100\n",
        "\n",
        "# train one model for/over each of the series\n",
        "for series_index in range(num_of_series):\n",
        "    # keep series name\n",
        "    series_name = series_names[series_index, 0]\n",
        "\n",
        "    # convert input dataset for specific time series to array of series values\n",
        "    series_values = dataset.iloc[[series_index], cols].values\n",
        "\n",
        "    # reshape to column array.  Rows are time series values at specific time points\n",
        "    series_values = series_values.reshape(-1,1)\n",
        "\n",
        "    # execute training, plots\n",
        "    execute(series_values, series_names[series_index:], w, load_trained_model, series_index)\n",
        "\n",
        "\n",
        "# train one model over all series\n",
        "\n",
        "# convert input dataset for first n time series to array of series values\n",
        "series_values = dataset.iloc[:num_of_series, cols].values\n",
        "\n",
        "# get transpose to have columns as different time series and rows as time series values at specific time points\n",
        "series_values = series_values.T\n",
        "\n",
        "# execute training, plots\n",
        "execute(series_values, series_names, w, load_trained_model, None)\n"
      ],
      "metadata": {
        "id": "12Mlwx3bJtQV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}